{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from sklearn\n",
    "data = load_breast_cancer()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome variable must be binary\n",
    "f'The outcome variable is {np.unique(data.target)} meaning the tumor could be {np.unique(data.target_names)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from the data\n",
    "df = pd.DataFrame(data.data, columns = data.feature_names)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the columns as features and investigate at the same time whether they have multicollinearity or not (high correlation)\n",
    "columns = df.columns.to_list()[:10]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 6])\n",
    "sns.heatmap(df[columns].corr(), annot=True, cmap = 'coolwarm', linewidths=2, linecolor='white')\n",
    "plt.title('Heatmap')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(columns[:2]+columns[4:5]+columns[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a heatmap in order to decide what features must be dropped to avoid multicollinearity\n",
    "plt.figure(figsize=[7, 5])\n",
    "sns.heatmap(df[features].corr(), annot=True, cmap=\"mako\", linewidths=2, linecolor='white')\n",
    "plt.title('Heatmap')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make X and y as predictor(df for the first run) and outcome(one D array) variables to the model\n",
    "X = df[features]\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_class_size = min(np.sum(data.target == 0), np.sum(data.target == 1))\n",
    "# At a maximum, there should be no more than the smallest class size divided by 10 number of features.\n",
    "max_features = min_class_size / 10\n",
    "f'''The maximum of the features is {max_features} and the number of predictor variables is {X.columns.nunique()\n",
    "}, so the assumption that the sample must be big enough is {max_features > X.columns.nunique()}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pairplot just for fun\n",
    "sns.pairplot(X, kind='scatter')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the X values by initalizing the StandardScaler then fit and transform the dataframe (X) back into a 2D array\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), type(y), X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a LogisticRegression model, fit the training X and y values and then predict y values with using test x values\n",
    "lrm = LogisticRegression(penalty=None, fit_intercept=True)\n",
    "model = lrm.fit(X_train, y_train)\n",
    "threshold = 0.25\n",
    "y_pred = np.where(model.predict_proba(X_test)[:, 1] > threshold, 1, 0)\n",
    "y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by using from sklearn.metrics import confusion_matrix and accuracy_score\n",
    "#print(f'The prediction threshold is: {threshold}')\n",
    "#print(f'''The confusion matrix is below:\\n\\n{confusion_matrix(y_test, y_pred)}\n",
    "#\\nThis represents the true positives, false positives in the first row, then false negatives and true negatives in the second row.\\n''')\n",
    "#print(f'The model is {round(100*accuracy_score(y_test, y_pred))}% accurate')\n",
    "#print(f'The model is {round(100*precision_score(y_test, y_pred))}% precise')\n",
    "#print(f'The model recall ratio is {round(100*recall_score(y_test, y_pred))}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.25, 0.5, 0.75]\n",
    "for threshold in thresholds:\n",
    "    y_pred = np.where(model.predict_proba(X_test)[:, 1] > threshold, 1, 0)\n",
    "# Evaluate the model by using from sklearn.metrics import confusion_matrix and accuracy_score\n",
    "    print(f'''The prediction threshold is: {threshold}\n",
    "Confusion matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\n",
    "Model accuracy: {round(100*accuracy_score(y_test, y_pred))}%\n",
    "Model precision: {round(100*precision_score(y_test, y_pred))}%\n",
    "Model recall ratio: {round(100*recall_score(y_test, y_pred))}%\\n\\n''')\n",
    "print('\\nConfusion matrix represents true positives, false positives in the first row, and false negatives, true negatives in the second row.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "`We can see that using lower prediction threshold means that we decrease not just the true positive count but also the number of false negatives.\n",
    "It means that we made the model more sensitive in order to save lives and catch as much malignant cancer as possible. \n",
    "The precision of the model is secondary but it has not changed significantly.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
